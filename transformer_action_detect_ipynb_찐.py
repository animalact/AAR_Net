# -*- coding: utf-8 -*-
"""transformer_action_detect.ipynb 찐

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuct3nUPrXp8XTa8pNop-i4WEkFWsmBn

#**Library**
"""

import math
import torch
import os
import time

import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn


'/content/drive/Shareddrives/예림 하이/label_yerim.tar'

"""# getAngle

"""
USE_CUDA = torch.cuda.is_available()
device = torch.device('cuda:0' if USE_CUDA else 'cpu')

def getAngle(start, mid, end):
        """
        Calculate the angle between start and end
        :param start: start point [x, y]
        :param end: mid point [x, y]
        :param end: end point [x, y]
        :return: the clockwise angle from start to end
        """
        v1 = np.array([0,0])
        v2 = np.array([0,0])

        v1[0] = (start[0]-mid[0])
        v1[1] = (start[1]-mid[1])

        v2[0] = (end[0]-mid[0])
        v2[1] = (end[1]-mid[1])
       #  angle = np.arccos((v1[0]*v2[0]+v1[1]*v2[1])/(math.sqrt(v1[0]*v1[0] + v1[1]*v1[1])*math.sqrt(v2[0]*v2[0] + v2[1]*v2[1]))*math.pi/180)

        #a = (v1[0]*v2[0]+v1[1]*v2[1])
        k = v1[0]*v1[0] + v1[1]*v1[1]
        j = v2[0]*v2[0] + v2[1]*v2[1]
        if k==0 or j==0 :
          angle = 0
        else:
          b = math.sqrt(k)*math.sqrt(j)
          c = np.inner(v1,v2)/b
          angle = 0
          if c <= 1 and c >=-1 :
            angle = np.arccos(c)
        #  print(c)
        #  print('각도:')
       #   print(angle*180/math.pi)
        
        return angle*180/math.pi

"""#**dataset.py**
act_id: 1 | 납작 엎드림 | 1103

act_id: 2 | 발을 숨기고 웅크리고 앉음 | 1467

act_id: 3 | 그루밍함 | 5180

act_id: 4 | 좌우로 뒹굴음 | 666

act_id: 5 | 머리를 들이댐 | 865

act_id: 6 | 배를 보임 | 937

act_id: 7 | 꼬리를 흔든다 | 3106

act_id: 8 | 앞발로 꾹꾹 누름 | 769

act_id: 9 | 허리를 아치로 세움 | 180

act_id: 10 | 옆으로 누워 있음 | 1731

act_id: 11 | 팔을 뻗어 휘적거림 | 2944

act_id: 12 | 걷거나 뛰는 동작 | 2557


"""

class trainDataset(torch.utils.data.Dataset):

    def __init__(self, data_path='./data'):
        train_dir_list = ['label_7', 'label_8', 'label_9']
        label_dir_list = ['label_7.csv', 'label_8.csv', 'label_9.csv']
        self.data_path = data_path

        self.exclude_labels = ["1","2","3","4","5","6","8","9","10"]
        self.label_num = 12 - len(self.exclude_labels)
        self.count_limit = 2500

        self.angle_sets = [[2,26,28],[2,8,26],[8,10,14],[8,12,16],[22,18,26],[24,20,26]]
        self.angle_num = len(self.angle_sets)

        self.label_map = {}
        self.label_key = {}

        # 제외할 라벨 및 개수 제한 
        self._dataFlattening(label_dir_list)


    def __len__(self):
        return len(self.train_videos)

    def __getitem__(self, idx):
        filename, label = self.train_videos[idx]
        video_data = np.load(os.path.join(self.data_path, "label_yerim", filename))
   #     angle_lists = []
   #     for angle_set in self.angle_sets:
        angle_list = self._getAngle(video_data)
   #         angle_lists.append(angle_list)
   #     item = (np.array(angle_lists), label)
        item = (angle_list, label)
        #print(item)
        return item

    def _dataFlattening(self, label_dir_list):
        import csv

        print("creating dataset")

        self.vid_dict = {}      # {"3": ['021314-cat-grooming.mp4', ... ], ... }
        self.act_dict = {}      # {"3": "그루밍 함", ...}
        for csv_file in label_dir_list:
            filepath = os.path.join(self.data_path, "label_yerim", csv_file)
            with open(filepath, "r") as f:
                reader = csv.DictReader(f)
                for info in reader:
                    act_id = info['action_id']

                    # * action이랑 인덱스 매칭 (필수는 아니지만 시각적인 도움)
                    if not self.act_dict.get(str(act_id), None):
                        self.act_dict[str(act_id)] = info['action']

                    # * self.exclude_labels 안에 있는 데이터들은 제외해주기!!
                    if act_id in self.exclude_labels:
                        if not isinstance(self.vid_dict.get(str(act_id), None), list):
                            self.vid_dict[str(act_id)] = []
                        continue
                    vid_list = self.vid_dict.get(act_id, [])

                    # 갯수 제한 있을 경우
                    if self.count_limit:
                        if len(vid_list) >= self.count_limit:
                            continue

                    filename = info['filename']
                    src = csv_file.split(".")[0]
                    filename = os.path.join(src, filename+".npy") # label_7/203030_cat-grooming-1234.mp4.npy

                    vid_list.append(filename)
                    self.vid_dict[act_id] = vid_list

        # 데이터를 줄였을때, 라벨링 값의 범위를 개수에 맞춰 줄여주기 위한 코드 (안줄이면 label 범위 (1,12), 3개 줄이면 (1, 9))
        # 주의! 이 개수에 맞춰 모델구조 다시 구성해줘야함
        # self.label_map = {'3': (0, '그루밍함'), '12': (1, '걷거나 뛰는 동작'), '7': (2, '꼬리를 흔든다'), '11': (3, '팔을 뻗어 휘적거림')}
        self.train_videos = []
        label = 0
        for i, k in enumerate(self.vid_dict):
            # ("13213-cat-grooming.mp4.npy", label:int) 형태로 리스트에 추가!
            for vid in self.vid_dict[k]:
                if not self.label_map.get(str(k), None):
                    self.label_map[str(k)] = (label, self.act_dict[str(k)], str(k))
                    self.label_key[str(label)] = str(k)
                    label += 1
                self.train_videos.append((vid, self.label_map[str(k)][0]))

        print(self.label_map)
        print(self.train_videos)
        self._showActdict()

    def _showActdict(self):
        for i in range(1,13):
            print(f"act_id: {str(i)} | {self.act_dict[str(i)]} | {len(self.vid_dict[str(i)])}")
            

    def _getAngle(self, video_data):
        angle_list = []
        angle_sett = [4,9,10]
        s, m, f = angle_sett[0], angle_sett[1] , angle_sett[2]

        '''
keypoints 1  2   3   4    5   6         7       8       9         10        11      12      13        14      15
         코 머리 입 입끝 목 앞우관절 앞좌관절 앞우발끝 앞좌발끝 뒤우관절 뒤좌관절 뒤우발끝 뒤좌발끝 꼬리시작 꼬리끝
          0    2   4  6    8    10      12         14     16        18      20        22        24     26    28
         다리각도 정도랑/ 꼬리/ 머리 목 다리/
        '''

        for i in range(0, 30):
            start_point = list()
            mid_point = list()
            end_point = list()
            if video_data[i][s] != 0:
                start_point.append(video_data[i][s])
                start_point.append(video_data[i][s+1])
            else:
                if len(angle_list) == 0:
                    angle_list.append(0)
                else :
                    angle_list.append(angle_list[-1])

                continue;
            ####   
            if video_data[i][m] != 0:
                mid_point.append(video_data[i][m])
                mid_point.append(video_data[i][m+1])
            else:
                if len(angle_list) == 0:
                    angle_list.append(0)
                else :
                    angle_list.append(angle_list[-1])
                        
                continue;
            #####    
            if video_data[i][f] != 0:
                end_point.append(video_data[i][f])
                end_point.append(video_data[i][f+1])
            else:
                if len(angle_list) == 0:
                    angle_list.append(0)
                else :
                    angle_list.append(angle_list[-1]) 

                continue;
            ######    
            angle = getAngle(start_point, mid_point, end_point)
            angle_list.append(angle)

        return angle_list

"""#**model.py**"""

class TransformerModel(nn.Module):
  def __init__(self, input_size, nhead, hidden_size, n_layers, dropout=0.1):
    super(TransformerModel, self).__init__()


    self.model_type = 'Transformer'

    self.sequence_length = 30
    self.input_size = input_size

  ### 인코더
    encoder_layers = nn.TransformerEncoderLayer(
        d_model=input_size, nhead=nhead, dim_feedforward=hidden_size, dropout=dropout, activation='relu', batch_first=True
    )


    self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=n_layers)

##디코더
    self.customed_decoder = nn.Sequential(
        nn.Linear(30, 128),
        nn.ReLU(),
        nn.Linear(128, 128),
        nn.ReLU(),
        nn.Linear(128, 3)
    )

  def forward(self, angle_seq):
  #  print(angle_seq)
    features= angle_seq.float()
    features = torch.unsqueeze(angle_seq, axis=1)

    output = self.transformer_encoder(features)

    output = torch.squeeze(output, axis=1)
 #   print(output.shape)
    output = self.customed_decoder(output)
    
#    print(output)

    return output

"""#**train valid test**"""

def train(dataloader, epoch):
    print('[Epoch %d]' % (epoch+1))

    criterion = nn.CrossEntropyLoss() 
    running_loss = 0.0

    start_time = time.time()
    for batch_idx, samples in enumerate(dataloader):
        # le = preprocessing.LabelEncoder()
        
        # labels = le.fit_transform(samples[1])
        labels = samples[1]
        labels = torch.tensor(labels)

        angle_list = torch.stack(samples[0], dim=1)

        angle_list, labels = angle_list.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad() 
        # forward + backward + optimize
        outputs = net(angle_list.float())
        loss = criterion(outputs, labels) 
      #  print(outputs, labels)
      #  print(loss)
        if not torch.isfinite(loss):
          print('WARNING: non-finite loss, ending training!')
          exit(1)

        loss.backward() 
        optimizer.step() 

        # print statistics
        running_loss += loss.item()
        if batch_idx % args.loss_interval == (args.loss_interval-1):
            elapsed = time.time() - start_time
            print('[%d, %5d] loss: %.6f in %.3f seconds' %
                  (epoch + 1, batch_idx + 1, running_loss / args.loss_interval, elapsed))
            
            running_loss = 0.0
            start_time = time.time()

        if args.split_training:
            if batch_idx > 10*args.loss_interval:
                break
    return running_loss / args.loss_interval

def valid(dataloader, epoch, output_size):
    print('**Validation**')
    criterion = nn.CrossEntropyLoss()
    valid_loss = 0.0

    classes = (0,1,2)
    #,13,14,15,16,17,18,19,20,21,22,23,24,25, 26)
    class_correct = list(0. for i in range(output_size))
    class_total = list(0. for i in range(output_size))
    class_acc = list(0. for i in range(output_size))

    with torch.no_grad():
        for batch_idx, samples in enumerate(dataloader):

            labels = samples[1]
            labels = torch.tensor(labels)

            angle_list = torch.stack(samples[0], dim=1)

            angle_list, labels = angle_list.to(device), labels.to(device)


            angle_list, labels = angle_list.to(device), labels.to(device)

            # forward + backward + optimize
            outputs = net(angle_list.float())
            # check if got error CUDA_LAUNCH_BLOCKING=1.
            loss = criterion(outputs, labels)

            valid_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            c = (predicted == labels).squeeze()
            for i in range(min(len(labels), args.batch_size)):
                label = labels[i]
                class_correct[label] += c[i].item()
                class_total[label] += 1

        hap = 0
        for i in range(output_size):
            if class_total[i] == 0:
                continue;
            class_acc[i] = 100 * class_correct[i] / class_total[i]
            print('Accuracy of %5s : %.2f %%' % (classes[i], class_acc[i]))
            hap += class_acc[i]

        valid_loss = valid_loss / len(dataloader)
        total_acc = hap / 3
        print('Accuracy of %5s : %.2f %%' % ('total', total_acc))
        print('Validation Loss: %.6f' % valid_loss)

      #  calculate valid best
        if total_acc > valid_best['total_acc']:
            valid_best['total_acc'] = total_acc
            valid_best['epoch'] = epoch + 1

            # save model
            torch.save(net.state_dict(), './model/transformer-action-best-detect.pth')

    return valid_loss, total_acc

"""#**<main**>"""

import easydict
from sklearn.model_selection import train_test_split

args = easydict.EasyDict({
        "batch_size": 4,
        "epoch": 500,
        "loss_interval": 500,
        "split_training": False,
        "data_path": './data',
        "model_name": 'transformer-action-detect.pth',
        "model_path": './output/'})

dataset = trainDataset(args.data_path)
train_dataset, valid_dataset = train_test_split(dataset,  test_size=0.2, shuffle=True, random_state=34)

'''
        코 머리 입 입끝 목 앞우관절 앞좌관절 앞우발끝 앞좌발끝 뒤우관절 뒤좌관절 뒤우발끝 뒤좌발끝 꼬리시작 꼬리끝
        0    2   4  6    8    10      12         14     16        18      20        22        24     26    28

[8, 26,28] 목 - 꼬리시작- 꼬리끝
'''




if __name__ == "__main__":
    label1 = []
    label2 = []
    label3 = []
    print(len(train_dataset))
    for i in range(6000):
        a = train_dataset[i]
        if a[1] == 0:
            label1.append(a[0])
        if a[1] == 1:
            label2.append(a[0])
        if a[1] == 2:
            label3.append(a[0])

    for i in range(1500) :
        walk_run = label1[i]
        tail = label2[i]
        arm = label3[i]

        print(walk_run)

        plt.subplot(221)
        plt.plot(walk_run, 'red')

        plt.subplot(222)
        plt.plot(tail, 'blue')

        plt.subplot(223)
        plt.plot(arm, 'green')

        plt.show()



